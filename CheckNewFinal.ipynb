{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import scipy\n",
    "import operator\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten\n",
    "#from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#устанавливаем seed\n",
    "np.random.seed(42)\n",
    "import pymorphy2\n",
    "import re\n",
    "import datetime\n",
    "import sklearn.feature_extraction.text as fex\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from IPython.core.display import display, HTML, Javascript, DisplayObject, display\n",
    "import lime\n",
    "import sklearn.pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import tomita\n",
    "#from tparser import tomita\n",
    "from tomita_parser import text_parse\n",
    "import pandas as pd\n",
    "def normalize(text):\n",
    "    return ' '.join([morph.parse(str(w))[0].normal_form for w in text.split()])\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) #deleting newlines and line-breaks\n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-\\'>><`', ' ', text) #deleting symbols  \n",
    "    text = re.sub('-', ' ', text) #deleting symbols  \n",
    "    #text = ' '.join(word[:] for word in text.split() if len(word)>3)\n",
    "    #text = text.encode(\"utf-8\")\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('dataset1.csv', delimiter=',', encoding='utf-8', error_bad_lines=False)\n",
    "#df_trainR = pd.read_csv('datasetNormFinal.csv', delimiter=',', encoding='utf-8', error_bad_lines=False)\n",
    "df_trainR = pd.read_csv('datasetNormFinal_v2.csv', delimiter=',', encoding='utf-8', error_bad_lines=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "textColName = \"TEXT\"\n",
    "negatColName =  \"DRAWBACKS\"\n",
    "positColName = \"BENEFITS\"\n",
    "ratingColName = \"RATING\"\n",
    "catColName = \"CATEGORY_ID\"\n",
    "brandColName = \"BRAND_ID\"\n",
    "catNameColName= \"CATEGORY_NAME\"\n",
    "tovarIdColName = \"PRODUCT\"\n",
    "fTextColName = 'fullText'\n",
    "fTextColNameNorm = 'fullTextNorm'\n",
    "tomitaColName = 'tomitaCol'\n",
    "normPosColName = \"NormPos\"\n",
    "normNegColName = \"NormNeg\"\n",
    "normTextColName = 'NormCpom'\n",
    "prodNameColName = 'NAME'\n",
    "#df_train[prodNameColName]\n",
    "with open('noun_adjf_classifiers.pkl', 'rb') as fid:\n",
    "    f = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train=df_trainR.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#category_data = df_train[df_train[catNameColName].str.contains(\"MARTPHON\")==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "prodIdList = [30025022,30025023,30024754,30026026]\n",
    "\n",
    "testList = df_train[(df_train[tovarIdColName] == prodIdList[0])]\n",
    "print(len(testList))\n",
    "\n",
    "for prId in prodIdList[1:]:\n",
    "    testList = pd.concat([testList,df_train[(df_train[tovarIdColName] == prId)]])\n",
    "print(len(testList))\n",
    "testList.to_csv(\"newProd2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270941\n",
      "270802\n"
     ]
    }
   ],
   "source": [
    "print(len (df_train))\n",
    "for prId in prodIdList:\n",
    "    df_train = df_train[(df_train[tovarIdColName] != prId)] \n",
    "    #                & (df_train[tovarIdColName] != 30025023 )]\n",
    "print(len (df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('newProd2.csv', delimiter=',', encoding='utf-8', error_bad_lines=False)\n",
    "colList = pd.unique(df_test[catNameColName])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "wtw = gensim.models.Word2Vec.load('word2vec_mvideo.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_feature_vector(words, model, num_features):\n",
    "        #function to average all words vectors in a given paragraph\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        #list containing names of words in the vocabulary\n",
    "        index2word_set = set(model.wv.index2word)\n",
    "        #this is moved as input param for performance reasons\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "        if(nwords>0):\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.607673750059\n"
     ]
    }
   ],
   "source": [
    "#get average vector for sentence 1\n",
    "from scipy import spatial\n",
    "#from  spatial import distance\n",
    "sentence_1 = \"телефон хороший\"\n",
    "sentence_1_avg_vector = avg_feature_vector(sentence_1.split(), model=wtw, num_features=100)\n",
    "\n",
    "#get average vector for sentence 2\n",
    "sentence_2 = \"телефон обязательно покупайте\"\n",
    "sentence_2_avg_vector = avg_feature_vector(sentence_2.split(), model=wtw, num_features=100)\n",
    "\n",
    "sen1_sen2_similarity =  1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n",
    "#print(sen1_sen2_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentSimilar(sent1, sent2):\n",
    "    sentence_1_avg_vector = avg_feature_vector(normalize(sent1).split(), model=wtw, num_features=100)\n",
    "    sentence_2_avg_vector = avg_feature_vector(normalize(sent2).split(), model=wtw, num_features=100)\n",
    "    res= 1 - spatial.distance.cosine(sentence_1_avg_vector,sentence_2_avg_vector)\n",
    "    return res\n",
    "#s1 = \"ИГРЫ ИДУТ БЕЗ ЛАГОВ ПОЛНОСТЬЮ\"\n",
    "#s2 = \"АППАРАТОМ ПОЛНОСТЬЮ ДОВОЛЕН\"\n",
    "#sentSimilar(normalize(s1),normalize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxFeatures = 10\n",
    "maxWft = 4\n",
    "maxPerSub = 3\n",
    "coefN = float(0.7)\n",
    "coefS = float(0.7)\n",
    "topCount = 15\n",
    "def word_weight(cv, explainer, words):\n",
    "    \"\"\"word - текст (строка)\"\"\"\n",
    "   # words = [str(w) for w in words if w is not None]\n",
    "    #print(\"call\")\n",
    "    words = \" \".join(words.split(\" \"))\n",
    "    #print(\"splitted\")\n",
    "    exp = explainer.explain_instance(words, cv.predict_proba, num_features=100)\n",
    "    return {k:v for k,v in exp.as_list()}\n",
    "        \n",
    "def printForCat(df_train, df_test,cat, fMod):\n",
    "    cv = sklearn.pipeline.make_pipeline(fMod[1], fMod[0])\n",
    "    explainer = LimeTextExplainer(class_names=[0,1])\n",
    " \n",
    "    category_data = df_train[df_train[catNameColName].str.contains(cat)==True].copy()\n",
    "    category_data[fTextColName] = category_data[textColName].astype('str') + \" \"+category_data[positColName].astype('str')\n",
    "    category_data[fTextColName] = category_data[fTextColName]  +\" \"+category_data[negatColName].astype('str')\n",
    "\n",
    "    if (normTextColName in df_train.columns):\n",
    "       # print (\"data already normalized\")\n",
    "        category_data[fTextColNameNorm] = category_data[normTextColName].astype('str') + \" \"+category_data[normNegColName].astype('str')\n",
    "        category_data[fTextColNameNorm] = category_data[fTextColNameNorm]  +\" \"+category_data[normPosColName].astype('str')\n",
    "    else:\n",
    "        print(\"start normalize at\" + str (datetime.datetime.now()))\n",
    "        category_data[fTextColNameNorm] = category_data[fTextColName].apply(clean_text).apply(normalize)\n",
    "        print(\"finishaed  collect normalize at \" + str (datetime.datetime.now().time()))\n",
    "    category_data_test = df_test[df_test[catNameColName].str.contains(cat)==True].copy()\n",
    "    category_data_test[fTextColName] = category_data_test[textColName].astype('str') + \" \"+category_data_test[positColName].astype('str')\n",
    "    category_data_test[fTextColName] = category_data_test[fTextColName]  +\" \"+category_data_test[negatColName].astype('str')\n",
    "    category_data_test[fTextColNameNorm]=category_data_test[fTextColName].apply(clean_text).apply(normalize)\n",
    "    textAndProdGrS_test = category_data_test.groupby(tovarIdColName )[tovarIdColName, fTextColName].agg(lambda x: \" %s \" % ', '.join(x)).reset_index()\n",
    "    textAndProdGrSNorm_test =  category_data_test.groupby(tovarIdColName )[tovarIdColName, fTextColNameNorm].agg(lambda x: \" %s \" % ', '.join(x)).reset_index()\n",
    "    textAndProdGrS = category_data.groupby(tovarIdColName )[tovarIdColName, fTextColName].agg(lambda x: \" %s \" % ', '.join(x)).reset_index()\n",
    "    textAndProdGrSNorm =  category_data.groupby(tovarIdColName )[tovarIdColName, fTextColNameNorm].agg(lambda x: \" %s \" % ', '.join(x)).reset_index()\n",
    "    textAndProdGrSNorm_All = pd.concat([textAndProdGrSNorm,textAndProdGrSNorm_test])\n",
    "    textAndProdGrS_All = pd.concat([textAndProdGrSNorm,textAndProdGrSNorm_test])\n",
    "   # print(\"total category items \" + str(len(textAndProdGrSNorm_All)))\n",
    "    tf_list = np.array(textAndProdGrSNorm_All)\n",
    "   # print(tf_list.shape)\n",
    "    id_tlist = textAndProdGrSNorm_test[tovarIdColName].tolist()\n",
    "\n",
    "    indList = [i for i in range(0,len(tf_list)) if tf_list[i][0] in id_tlist]\n",
    "    textAndProdGrS_test[tomitaColName] = textAndProdGrS_test[:][fTextColName].apply(text_parse)\n",
    "    tfidf = fex.TfidfVectorizer()\n",
    "    #print(\"start tfidf at\" + str (datetime.datetime.now().time()))\n",
    "    tfIdfMatrix = tfidf.fit_transform(tf_list[:,1])\n",
    "    #print(\"end tf Idf at \" + str (datetime.datetime.now().time()))\n",
    "    ftNames = tfidf.get_feature_names()\n",
    "    \n",
    "    densM = tfIdfMatrix.todense()\n",
    "    for ind in indList[:]:\n",
    "        print(\"#\"*50)\n",
    "        display(HTML('<h2>Продукт</h2>'))\n",
    "        f_weigth = word_weight(cv,explainer,tf_list[ind][1])\n",
    "        display(HTML(df_test[df_test[tovarIdColName] == tf_list[ind][0]][prodNameColName].iloc[0]))\n",
    "        col = densM[ind].tolist()[0]\n",
    "        valD = { ftNames[i]:v for i,v in enumerate(col) if v>0  }\n",
    "        sorted_topWords = sorted(valD.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_topWords = [ s for s in sorted_topWords if len(s[0])>3]\n",
    "        dict_r = { s[0]:s[1] for s in sorted_topWords}\n",
    "        tomitaD = textAndProdGrS_test[textAndProdGrS_test[tovarIdColName] == textAndProdGrS_All.iloc[ind][tovarIdColName]][tomitaColName]\n",
    "        sublist = list([x[::,0] for x in tomitaD.values][0])\n",
    "        set_ret = list()\n",
    "        tomitaPairs = tomitaD.tolist()[0]\n",
    "        #print(\"start word rating rebuild at \" + str (datetime.datetime.now().time()))\n",
    "        setSent = list()\n",
    "        setSentWord = dict()\n",
    "        top10List=list()\n",
    "        for ts in tomitaPairs:\n",
    "            skip=False\n",
    "\n",
    "\n",
    "            ret =0\n",
    "            retBonus = 0\n",
    "            relationRet = 0\n",
    "            tsn = normalize(ts[0])\n",
    "            if tsn in dict_r.keys():\n",
    "                ret+=dict_r[tsn]*float(coefS)\n",
    "            if tsn in f_weigth.keys():\n",
    "                #print(\"noon ok\")\n",
    "                relationRet+=f_weigth[tsn]*float(coefS)\n",
    "                if f_weigth[tsn]<0:\n",
    "                    relationRet+=f_weigth[tsn]*2\n",
    "                retBonus+=abs(1*f_weigth[tsn]*float(coefS))\n",
    "            retList = list()\n",
    "            for tsPr in ts[1].split(\" \"):\n",
    "                tsPrn = normalize(tsPr)\n",
    "                if tsPrn in dict_r.keys():\n",
    "                    retList.append(dict_r[tsPrn])\n",
    "                if tsPrn in f_weigth.keys():\n",
    "                    #print(\"not noon ok\")\n",
    "                    relationRet+=f_weigth[tsPrn]\n",
    "                    retBonus+=abs(1*f_weigth[tsPrn])\n",
    "                    if f_weigth[tsPrn]<0:\n",
    "                        relationRet+=f_weigth[tsPrn]*2\n",
    "\n",
    "            #ret+=retBonus\n",
    "            retList.sort(reverse = True)\n",
    "            coef = coefN\n",
    "            for retN in retList:\n",
    "                ret+=float(retN)*coef\n",
    "                coef*=coef\n",
    "            if len(top10List) < topCount:\n",
    "                top10List.append(ret)\n",
    "                top10List.sort(reverse=True)\n",
    "            else:\n",
    "                if ret <top10List[-1]:\n",
    "                    continue\n",
    "                top10List.append(ret)\n",
    "                top10List.sort(reverse=True)\n",
    "                del top10List[-1]\n",
    "            for sent in setSent:\n",
    "                #print(sent)\n",
    "                #print(ts[1])\n",
    "                #print( sentSimilar(sent,ts[1]))\n",
    "                if sentSimilar(sent,ts[1]) > float(0.8):\n",
    "                    skip=True\n",
    "                    break\n",
    "            if skip:\n",
    "                #print(sent)\n",
    "                #print(ts[1])\n",
    "                #print( sentSimilar(sent,ts[1]))\n",
    "                continue\n",
    "            setSent.append(ts[1])\n",
    "            \n",
    "            set_ret.append([ts[1],ret,ts[0],relationRet])\n",
    "        \n",
    "        #print(\"end word rating rebuild at \" + str (datetime.datetime.now().time()))\n",
    "        sorted_sent = sorted(set_ret, key=operator.itemgetter(1), reverse=True)\n",
    "        #print(sorted_sent)\n",
    "        for ft in sorted_sent:\n",
    "            if  str(ft[2]) in setSentWord.keys():\n",
    "                setSentWord[ft[2]]+=ft[1]\n",
    "            else:\n",
    "                setSentWord[ft[2]]=ft[1]\n",
    "        sorted_w_sent = sorted(setSentWord.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "               \n",
    "        subDict = dict()\n",
    "        curInd = 0\n",
    "        htmlStr = \"\"\"<table border=\"0\"> <tr> <th>Особенности</th>\n",
    "    <th>Важность</th>\n",
    "    <th>Отношение</th>\n",
    "    \n",
    "    </tr> \"\"\"\n",
    "        wholeText = \"\"\n",
    "        for feature in sorted_sent:\n",
    "            if  str(feature[2]) in subDict.keys():\n",
    "                subDict[feature[2]]+=1\n",
    "            else:\n",
    "                subDict[feature[2]]=1\n",
    "            if (subDict[feature[2]] < maxPerSub):\n",
    "                featText = list(feature[0].lower())\n",
    "                featText[0] = featText[0].upper()\n",
    "                featText = \"\".join(featText) \n",
    "                wholeText += \" \" + str(featText)\n",
    "                featRet = int(float(feature[3])*10000) \n",
    "                \n",
    "                relationText = \"Нейтральное\" if  featRet == 0 else (\"Хорошее\" if featRet>0 else \"Плохое\")\n",
    "                #+  featText.Substring(1);\n",
    "                htmlStr +=\" <tr><td> \"\n",
    "                htmlStr += \"  \"+featText+ \" </td><td>\"\n",
    "                htmlStr += \"  \"+str(int(float(feature[1])*100))+ \" </td><td>\"\n",
    "                htmlStr += \"  \"+relationText+ \" </td><tr>\"\n",
    "                \n",
    "\n",
    "                #print(\" особенность: \"+str(featText)+ \" важность \"+ \n",
    "                #      str(int(float(feature[1])*100)))\n",
    "                      #+ \"сущ\" + str(feature[2]))\n",
    "                curInd+=1\n",
    "                \n",
    "         \n",
    "            if curInd > maxFeatures:\n",
    "                break\n",
    "       # print(wholeText)\n",
    "        wholeText = ' '.join(word[:] for word in normalize(clean_text(wholeText)).split() if len(word)>3)\n",
    "\n",
    "        wordcloud = WordCloud(max_font_size=40).generate(wholeText)\n",
    "        plt.figure(figsize=(35,35))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()                \n",
    "        display(HTML(htmlStr))\n",
    "        for we in sorted_w_sent[:maxWft]:\n",
    "            cnt=0\n",
    "            display(HTML('<h4>'+we[0]+'</h2>'))\n",
    "\n",
    "            for ft in sorted_sent:\n",
    "                if ft[2] == we[0]:\n",
    "                    cnt+=1\n",
    "                    featText = list(ft[0].lower())\n",
    "                    featText[0] = featText[0].upper()\n",
    "                    featText = \"\".join(featText)\n",
    "                    featRet = int(float(ft[3])*10000) \n",
    "                    relationText = \"Нейтрально: \" if  featRet == 0 else (\"Хорошо: \" if featRet>0 else \"Плохо: \")\n",
    "                    featText= relationText + featText\n",
    "                    print(featText)\n",
    "                    if cnt > maxPerSub:\n",
    "                        break\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_category(string):\n",
    "    if 'TV' in string and 'COMMERC' not in string:\n",
    "        return 'tv'\n",
    "    elif 'REFRIGERATOR' in string:\n",
    "        return 'fridge'\n",
    "    elif 'VACUUM' in string and 'CLEAN' in string:\n",
    "        return 'cleaner'\n",
    "    elif 'WASHING' in string and 'MACHINE' in string and 'ACCESSOR' not in string:\n",
    "        return 'washmachine'\n",
    "    elif 'NOTEBOOK' in string or 'LAPTOP' in string and  'ACCESSOR' not in string:\n",
    "        return 'laptop'\n",
    "    elif 'SMARTPHONE' in string and  'ACCESSOR' not in string:\n",
    "        return 'phone'\n",
    "    elif 'DISHWASHER' in string and  'ACCESSOR' not in string and 'FILTER' not in string:\n",
    "        return 'dishwasher'\n",
    "    print (\"problem with category \" + string)\n",
    "    return 'phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>SMARTPHONES                               </h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Продукт</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Смартфон Huawei Honor 4C Pro Grey (TIT-L01)                                                                  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat = colList[0][1:]\n",
    "#print(colList)\n",
    "for cat in colList[:1]:\n",
    "    fMod = f[string_to_category(cat)]\n",
    "    #print(fMod[1])\n",
    "    display(HTML('<h1>'+str(cat) +'</h1>'))\n",
    "    printForCat(df_train, df_test,cat[1:],fMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#listN = dfTest.tolist()\n",
    "import gensim\n",
    "import pandas as pd\n",
    "entries = [[str(w) for w in e.split()] for e in df.NormCpom]\n",
    "wtw = gensim.models.Word2Vec(entries, size=100, window=5, min_count=5, workers=4)\n",
    "wtw.save('word2vec_mvideo.model')\n",
    "#print(wtw.wv.most_similar(positive=['хороший'], negative=['плохой']))\n",
    "#print(wtw.similarity('отличный', 'ужасный'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
