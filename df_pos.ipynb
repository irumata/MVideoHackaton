{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import itertools \n",
    "import functools\n",
    "import nltk\n",
    "import sklearn.feature_extraction.text as fex\n",
    "import scipy.sparse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, precision_score,recall_score, median_absolute_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import regression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "def get_pos_columns(df):\n",
    "    return df[['noun', 'adjf', 'advb', 'verb', 'PROCESSED', 'PRODUCT', 'NAME', 'CATEGORY_ID', \n",
    "              'BRAND_ID', 'MYCATEGORY', 'CATEGORY_NAME', 'good_bad_ugly', 'BENEFITS', 'DRAWBACKS', 'RECOMMENDED',\n",
    "              'LIKES_COUNT', 'DISLIKES_COUNT', 'RATING']]\n",
    "\n",
    "\n",
    "def get_vocabulary_for_category(comment_by_category, min_df, max_df):\n",
    "    words_by_cat = {k: df_comment_to_normal_words(v, [prepate_nots]) for k, v in comment_by_category.items()}\n",
    "    vectorizers = {k: fex.TfidfVectorizer(min_df=min_df,\n",
    "                                    max_df=max_df,\n",
    "                                    sublinear_tf=True,\n",
    "                                    use_idf=True) for k, v in words_by_cat.items()} \n",
    "\n",
    "    ready_vectorizers = {k: vectorizers[k].fit_transform(words_by_cat[k]) for k in words_by_cat.keys()}\n",
    "\n",
    "    stop_words_ext = stop_words.union({'пылесос', 'это', 'очень', 'моет', 'посуду', 'это', 'очень', 'холодильник', 'очень', 'работает', 'это'\n",
    "                     'холодильник', 'это', 'очень', 'работает', 'телефон', 'машинка', 'машинки', 'телевизор', 'стирает',\n",
    "                                      'машинку', 'качество', 'ноутбук'})\n",
    "\n",
    "    return {k: [i for i in sorted(v.vocabulary_.keys()) if i not in stop_words_ext] for k,v in vectorizers.items()}\n",
    "\n",
    "def split_df_by_category(df, column_name):\n",
    "    return {v: df[df[column_name]==v] for v in df[column_name].value_counts().keys()}\n",
    "\n",
    "def classification(train, min_df, max_df, inp, out):\n",
    "    vectorizer = fex.TfidfVectorizer(min_df=min_df, max_df=max_df, sublinear_tf=True, use_idf=True)\n",
    "    words = [str(i) for i in train[inp]]\n",
    "    rating = np.array(train[out])\n",
    "    \n",
    "    div_ratio = 0.9\n",
    "    train_words, test_words = np.split(words, [int(div_ratio*len(words))])\n",
    "    train_rating, test_rating = np.split(rating, [int(div_ratio*len(rating))])\n",
    "    train_vectors = vectorizer.fit_transform(train_words).todense()\n",
    "    test_vectors = vectorizer.transform(test_words).todense()\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1,\n",
    "                                       min_samples_leaf=int(len(train_rating)/2000))\n",
    "    classifier.fit(train_vectors, train_rating)\n",
    "    \n",
    "    return classifier, vectorizer, train_vectors, test_vectors, train_rating, test_rating\n",
    "\n",
    "def nan_or_importance(word, key, importance):\n",
    "    imp = {k: v for k,v in importance[key]}\n",
    "    keys = imp.keys()\n",
    "    return np.nan if word not in keys else imp[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_df = pd.read_csv('/home/misha/stop_words.csv')\n",
    "stop_words = set([str(w) for w in stop_words_df.word])\n",
    "\n",
    "df_1 = pd.read_csv('/home/misha/Downloads/POS1.csv')\n",
    "df_2 = pd.read_csv('/home/misha/Downloads/POS2.csv')\n",
    "df_3 = pd.read_csv('/home/misha/Downloads/POS3.csv')\n",
    "# Собирается датасет из частей (части считал отдельно в документах тк не паралелится)\n",
    "df = pd.concat([get_pos_columns(df) for df in [df_1, df_2, df_3]])\n",
    "# Словарь название товара: ключ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем части речи по категориям\n",
    "noun_by_category = {k: v.noun for k, v in split_df_by_category(df, 'good_bad_ugly').items()}\n",
    "adjf_by_category = {k: v.adjf for k, v in split_df_by_category(df, 'good_bad_ugly').items()}\n",
    "advb_by_category = {k: v.advb for k, v in split_df_by_category(df, 'good_bad_ugly').items()}\n",
    "verb_by_category = {k: v.verb for k, v in split_df_by_category(df, 'good_bad_ugly').items()}\n",
    "\n",
    "# создаем словари категория: набор слов\n",
    "voc_nouns = get_vocabulary_for_category(noun_by_category, 0.002, 0.75)\n",
    "voc_adjfs = get_vocabulary_for_category(adjf_by_category, 0.002, 0.75)\n",
    "voc_advbs = get_vocabulary_for_category(advb_by_category, 0.002, 0.75)\n",
    "voc_verbs = get_vocabulary_for_category(verb_by_category, 0.002, 0.75)\n",
    "\n",
    "category_keys = [i for i in df.MYCATEGORY.value_counts().keys()]\n",
    "dict_itemnum = {v: k for k, v in enumerate(df.NAME.value_counts().keys())}\n",
    "\n",
    "# категории 0-1 в плохой-хороший\n",
    "df['mark'] = df.good_bad_ugly.apply(lambda x: 0 if 'bad' in x else 1)\n",
    "\n",
    "# создаем датафрейм для обучения лесов, чтобы затем найти самые важные фичи. \n",
    "# Отдельный лес обучается на каждой группе товаров\n",
    "df_by_category = {key: df[df.MYCATEGORY==key][['noun', \n",
    "                                               'mark',\n",
    "                                               'RATING',\n",
    "                                               'good_bad_ugly',\n",
    "                                               'NAME',\n",
    "                                               'CATEGORY_ID']]\n",
    "                  for key in category_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_features = {}\n",
    "for key in category_keys:\n",
    "    c, v, tr_v, te_v, tr_e, te_e = classification(df_by_category[key], 0.01, 0.4, 'noun', 'mark')\n",
    "    category_features[key] = (c, v,  tr_v, te_v, tr_e, te_e)\n",
    "    \n",
    "prediction_fines = {}\n",
    "for key in category_keys:\n",
    "    c, v, tr_v, te_v, tr_e, te_e = category_features[key]\n",
    "    p = c.predict(te_v)\n",
    "    prediction_fines[key] = len([p[i] for i in range(len(te_e)) if p[i]==te_e[i]])/len(te_e)\n",
    "    \n",
    "feature_importances_by_category = {}\n",
    "for key in category_keys:\n",
    "    c = category_features[key][0]\n",
    "    v = category_features[key][1]\n",
    "    feat_imp = c.feature_importances_\n",
    "    vocab = v.vocabulary_\n",
    "    importance = {k: feat_imp[vocab[k]] for k in vocab.keys() }\n",
    "    importance_pairs = sorted([(k,v)for k,v in importance.items()], key=lambda x: x[1])\n",
    "    feature_importances_by_category[key] = importance_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tv</th>\n",
       "      <th>phone</th>\n",
       "      <th>laptop</th>\n",
       "      <th>cleaner</th>\n",
       "      <th>fridge</th>\n",
       "      <th>washmachine</th>\n",
       "      <th>dishwasher</th>\n",
       "      <th>allin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>соня</td>\n",
       "      <td>7.563169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>симка</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.345924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>тормоз</td>\n",
       "      <td>4.711287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.303652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>жена</td>\n",
       "      <td>3.390638</td>\n",
       "      <td>1.257638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.067374</td>\n",
       "      <td>0.662916</td>\n",
       "      <td>1.234252</td>\n",
       "      <td>3.447004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>параметр</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.137151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature        tv     phone    laptop   cleaner    fridge  washmachine  \\\n",
       "0      соня  7.563169  0.000000  0.000000  0.000000  0.000000     0.000000   \n",
       "1     симка  0.000000  5.345924  0.000000  0.000000  0.000000     0.000000   \n",
       "2    тормоз  4.711287  0.000000  1.303652  0.000000  0.000000     0.000000   \n",
       "3      жена  3.390638  1.257638  0.000000  3.067374  0.662916     1.234252   \n",
       "4  параметр  0.000000  0.000000  5.137151  0.000000  0.000000     0.000000   \n",
       "\n",
       "   dishwasher  allin  \n",
       "0    0.000000  False  \n",
       "1    0.000000  False  \n",
       "2    0.000000  False  \n",
       "3    3.447004  False  \n",
       "4    0.000000  False  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# собрать все слова для всех товаров в множество\n",
    "word_set = set([item[0] for sublist in feature_importances_by_category.values() for item in sublist])\n",
    "\n",
    "# проставить в колонки для разных товаров nan (если слово не встречается) или важность слова\n",
    "df_ = pd.DataFrame()\n",
    "df_['feature'] = pd.Series([w for w in word_set])\n",
    "for key in category_keys:\n",
    "    df_[key] = pd.Series([nan_or_importance(word, key, feature_importances_by_category) for word in word_set])\n",
    "    \n",
    "# Отметить фичи, которые встречаются во всех категориях, нормализовать на 0-100\n",
    "df_['allin'] = df_.notnull().all(axis=1)\n",
    "for key in category_keys:\n",
    "    df_[key] = (df_[key] - df_[key].min()) / (df_[key].max() - df_[key].min()) * 100\n",
    "\n",
    "#выбросиь такие фичи\n",
    "#df_clean = df[df.allin != True]\n",
    "df_ = df_.fillna(0)\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn.pipeline\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, v, tr_v, te_v, tr_e, te_e = classification(df_by_category['laptop'], 0.01, 0.4, 'adjf', 'mark')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn.pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "cv = sklearn.pipeline.make_pipeline(v, c)\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "def word_weight(cv, explainer, words):\n",
    "    \"\"\"word - текст (строка)\"\"\"\n",
    "    exp = explainer.explain_instance(words, cv.predict_proba, num_features=100)\n",
    "    return {k:v for k,v in exp.as_list}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "low >= high",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-704ad5eca064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexp_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mexp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-311-06eb1a374714>\u001b[0m in \u001b[0;36mword_weight\u001b[0;34m(cv, explainer, words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/misha/anaconda3/lib/python3.6/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    255\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[1;32m    256\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             distance_metric=distance_metric)\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/misha/anaconda3/lib/python3.6/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[0;34m(cls, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdoc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:16117)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "class_names=[0,1]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "cv = sklearn.pipeline.make_pipeline(v, c)\n",
    "\n",
    "words = df_by_category['laptop'].adjf\n",
    "words = [str(w) for w in words if w != np.nan and len(str(w)) > 0 ]\n",
    "\n",
    "exp_list = []\n",
    "for idx in range(4, 10):\n",
    "    exp_list.append(word_weight(cv, explainer, words[idx]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('noun_classifiers.pkl', 'wb') as fid:\n",
    "    pickle.dump({k: (v[0],v[1]) for k,v in category_features.items()}, fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# в файл, если надо\n",
    "for key in category_keys:\n",
    "    df = pd.DataFrame.from_records(feature_importances_by_category[key], columns=['feature', 'importance'])\n",
    "    df.to_csv('/home/misha/Downloads/{}.csv'.format(key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
